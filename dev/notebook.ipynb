{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15661f4c",
   "metadata": {},
   "source": [
    "# Load and Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9625f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a598430",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data'\n",
    "REVIEWS_FILE = 'olist_order_reviews_dataset.csv'\n",
    "ORDERS_FILE = 'olist_orders_dataset.csv'\n",
    "CUSTOMERS_FILE = 'olist_customers_dataset.csv'\n",
    "PROCESSED_DATA_FILE = 'processed_customer_data.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af56a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files loaded successfully!\n",
      "\n",
      "--- Reviews Data ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99224 entries, 0 to 99223\n",
      "Data columns (total 7 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   review_id                99224 non-null  object\n",
      " 1   order_id                 99224 non-null  object\n",
      " 2   review_score             99224 non-null  int64 \n",
      " 3   review_comment_title     11568 non-null  object\n",
      " 4   review_comment_message   40977 non-null  object\n",
      " 5   review_creation_date     99224 non-null  object\n",
      " 6   review_answer_timestamp  99224 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 5.3+ MB\n",
      "None\n",
      "                          review_id                          order_id  \\\n",
      "0  7bc2406110b926393aa56f80a40eba40  73fc7af87114b39712e6da79b0a377eb   \n",
      "1  80e641a11e56f04c1ad469d5645fdfde  a548910a1c6147796b98fdf73dbeba33   \n",
      "2  228ce5500dc1d8e020d8d1322874b6f0  f9e4b658b201a9f2ecdecbb34bed034b   \n",
      "3  e64fb393e7b32834bb789ff8bb30750e  658677c97b385a9be170737859d3511b   \n",
      "4  f7c4243c7fe1938f181bec41a392bdeb  8e6bfb81e283fa7e4f11123a3fb894f1   \n",
      "\n",
      "   review_score review_comment_title  \\\n",
      "0             4                  NaN   \n",
      "1             5                  NaN   \n",
      "2             5                  NaN   \n",
      "3             5                  NaN   \n",
      "4             5                  NaN   \n",
      "\n",
      "                              review_comment_message review_creation_date  \\\n",
      "0                                                NaN  2018-01-18 00:00:00   \n",
      "1                                                NaN  2018-03-10 00:00:00   \n",
      "2                                                NaN  2018-02-17 00:00:00   \n",
      "3              Recebi bem antes do prazo estipulado.  2017-04-21 00:00:00   \n",
      "4  Parabéns lojas lannister adorei comprar pela I...  2018-03-01 00:00:00   \n",
      "\n",
      "  review_answer_timestamp  \n",
      "0     2018-01-18 21:46:59  \n",
      "1     2018-03-11 03:05:13  \n",
      "2     2018-02-18 14:36:24  \n",
      "3     2017-04-21 22:02:06  \n",
      "4     2018-03-02 10:26:53  \n",
      "\n",
      "--- Orders Data ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99441 entries, 0 to 99440\n",
      "Data columns (total 12 columns):\n",
      " #   Column                         Non-Null Count  Dtype \n",
      "---  ------                         --------------  ----- \n",
      " 0   order_id                       99441 non-null  object\n",
      " 1   customer_id                    99441 non-null  object\n",
      " 2   order_status                   99441 non-null  object\n",
      " 3   order_purchase_timestamp       99441 non-null  object\n",
      " 4   order_approved_at              99281 non-null  object\n",
      " 5   order_delivered_carrier_date   97658 non-null  object\n",
      " 6   order_delivered_customer_date  96476 non-null  object\n",
      " 7   order_estimated_delivery_date  99441 non-null  object\n",
      " 8   customer_unique_id             99441 non-null  object\n",
      " 9   customer_zip_code_prefix       99441 non-null  int64 \n",
      " 10  customer_city                  99441 non-null  object\n",
      " 11  customer_state                 99441 non-null  object\n",
      "dtypes: int64(1), object(11)\n",
      "memory usage: 9.1+ MB\n",
      "None\n",
      "                           order_id                       customer_id  \\\n",
      "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
      "1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
      "2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
      "3  949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
      "4  ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
      "\n",
      "  order_status order_purchase_timestamp    order_approved_at  \\\n",
      "0    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
      "1    delivered      2018-07-24 20:41:37  2018-07-26 03:24:27   \n",
      "2    delivered      2018-08-08 08:38:49  2018-08-08 08:55:23   \n",
      "3    delivered      2017-11-18 19:28:06  2017-11-18 19:45:59   \n",
      "4    delivered      2018-02-13 21:18:39  2018-02-13 22:20:29   \n",
      "\n",
      "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
      "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
      "1          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
      "2          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
      "3          2017-11-22 13:39:59           2017-12-02 00:28:42   \n",
      "4          2018-02-14 19:46:34           2018-02-16 18:17:02   \n",
      "\n",
      "  order_estimated_delivery_date                customer_unique_id  \\\n",
      "0           2017-10-18 00:00:00  7c396fd4830fd04220f754e42b4e5bff   \n",
      "1           2018-08-13 00:00:00  af07308b275d755c9edb36a90c618231   \n",
      "2           2018-09-04 00:00:00  3a653a41f6f9fc3d2a113cf8398680e8   \n",
      "3           2017-12-15 00:00:00  7c142cf63193a1473d2e66489a9ae977   \n",
      "4           2018-02-26 00:00:00  72632f0f9dd73dfee390c9b22eb56dd6   \n",
      "\n",
      "   customer_zip_code_prefix            customer_city customer_state  \n",
      "0                      3149                sao paulo             SP  \n",
      "1                     47813                barreiras             BA  \n",
      "2                     75265               vianopolis             GO  \n",
      "3                     59296  sao goncalo do amarante             RN  \n",
      "4                      9195              santo andre             SP  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "try:\n",
    "    reviews_df = pd.read_csv(os.path.join(DATA_DIR, REVIEWS_FILE))\n",
    "    orders_df = pd.read_csv(os.path.join(DATA_DIR, ORDERS_FILE))\n",
    "    customers_df = pd.read_csv(os.path.join(DATA_DIR, CUSTOMERS_FILE))\n",
    "    \n",
    "    print(\"Files loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Check the CSV files are in the 'data/' directory.\")\n",
    "\n",
    "if 'reviews_df' in locals():\n",
    "    print(\"\\n--- Reviews Data ---\")\n",
    "    print(reviews_df.info())\n",
    "    print(reviews_df.head())\n",
    "\n",
    "if 'orders_df' in locals():\n",
    "    print(\"\\n--- Orders Data ---\")\n",
    "    \n",
    "    orders_customers_df = pd.merge(orders_df, customers_df, on='customer_id')\n",
    "    print(orders_customers_df.info())\n",
    "    print(orders_customers_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c30dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data merged successfully. Shape of merged data: (99224, 18)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Merge the datasets ---\n",
    "orders_customers_df = pd.merge(orders_df, customers_df, on='customer_id')\n",
    "\n",
    "# Merge the result with reviews to link orders to their reviews\n",
    "merged_df = pd.merge(orders_customers_df, reviews_df, on='order_id')\n",
    "\n",
    "print(\"Data merged successfully. Shape of merged data:\", merged_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee97ab5d",
   "metadata": {},
   "source": [
    "## Handle missing data and data types\n",
    "- For this project, we only care about reviews with actual comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693fbd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered for reviews with comments. New shape: (40977, 18)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "merged_df.dropna(subset=['review_comment_message'], inplace=True)\n",
    "\n",
    "# Convert timestamps to datetime\n",
    "merged_df['order_purchase_timestamp'] = pd.to_datetime(merged_df['order_purchase_timestamp'])\n",
    "\n",
    "print(\"Filtered for reviews with comments. New shape:\", merged_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166198c",
   "metadata": {},
   "source": [
    "## Define the transformation function\n",
    "- Convert structured data into a text sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_document_for_order(row):\n",
    "    \"\"\"\n",
    "    Creates a single text document for a given order (a row in the dataframe).\n",
    "    \"\"\"\n",
    "    # Convert structured data to natural language\n",
    "    purchase_date = row['order_purchase_timestamp'].strftime('%B %d, %Y')\n",
    "    customer_location = f\"{row['customer_city']}, {row['customer_state']}\"\n",
    "    \n",
    "    order_summary = (\n",
    "        f\"Customer {row['customer_unique_id']} from {customer_location} \"\n",
    "        f\"placed an order on {purchase_date}. \"\n",
    "        f\"The order status is '{row['order_status']}'. \"\n",
    "    )\n",
    "    \n",
    "    review_summary = (\n",
    "        f\"The customer left a review with a score of {row['review_score']} out of 5. \"\n",
    "        f\"The review comment is: '{row['review_comment_message']}'\"\n",
    "    )\n",
    "    \n",
    "    # Combine everything into one document\n",
    "    full_document = order_summary + review_summary\n",
    "    return full_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f6e2e",
   "metadata": {},
   "source": [
    "## Apply the function and create the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60564ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating text documents for each order...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerating text documents for each order...\")\n",
    "merged_df['text_document'] = merged_df.apply(create_document_for_order, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de606a48",
   "metadata": {},
   "source": [
    "## Create final Dataframe with required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = merged_df[['customer_unique_id', 'order_id', 'text_document']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53952c82",
   "metadata": {},
   "source": [
    "## Save the final dataset as a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52fe6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example of a generated document ---\n",
      "Customer 7c396fd4830fd04220f754e42b4e5bff from sao paulo, SP placed an order on October 02, 2017. The order status is 'delivered'. The customer left a review with a score of 4 out of 5. The review comment is: 'Não testei o produto ainda, mas ele veio correto e em boas condições. Apenas a caixa que veio bem amassada e danificada, o que ficará chato, pois se trata de um presente.'\n",
      "\n",
      "Processed data saved to 'processed_customer_data.parquet'.\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "print(\"\\n--- Example of a generated document ---\")\n",
    "print(final_df.iloc[0]['text_document'])\n",
    "\n",
    "table = pa.Table.from_pandas(final_df)\n",
    "pq.write_table(table, os.path.join(DATA_DIR, PROCESSED_DATA_FILE))\n",
    "\n",
    "print(f\"\\nProcessed data saved to '{PROCESSED_DATA_FILE}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543b661b",
   "metadata": {},
   "source": [
    "# Text Chunking & Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f659d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "DATA_DIR = '../data'\n",
    "PROCESSED_DATA_FILE = os.path.join(DATA_DIR, 'processed_customer_data.parquet')\n",
    "EMBEDDINGS_FILE = os.path.join(DATA_DIR, 'customer_embeddings.pkl')\n",
    "EMBEDDING_MODEL_NAME = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "\n",
    "\n",
    "df = pq.read_pandas(PROCESSED_DATA_FILE).to_pandas()\n",
    "print(f\"Loaded {len(df)} documents to be chunked and embedded.\")\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Loading embedding model: {EMBEDDING_MODEL_NAME}...\")\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device='cpu')\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a808f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 40977 documents to be chunked and embedded.\n",
      "Loading embedding model: paraphrase-multilingual-mpnet-base-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 0ccde8bf-7871-46bc-8e50-8eb14fa9f085)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "\n",
      "Starting chunking and embedding process...\n",
      "Processed 5000/40977 documents...\n",
      "Processed 9000/40977 documents...\n",
      "Processed 20000/40977 documents...\n",
      "Processed 21000/40977 documents...\n",
      "Processed 22000/40977 documents...\n",
      "Processed 25000/40977 documents...\n",
      "Processed 26000/40977 documents...\n",
      "Processed 27000/40977 documents...\n",
      "Processed 28000/40977 documents...\n",
      "Processed 31000/40977 documents...\n",
      "Processed 32000/40977 documents...\n",
      "Processed 41000/40977 documents...\n",
      "Processed 46000/40977 documents...\n",
      "Processed 50000/40977 documents...\n",
      "Processed 60000/40977 documents...\n",
      "Processed 61000/40977 documents...\n",
      "Processed 62000/40977 documents...\n",
      "Processed 64000/40977 documents...\n",
      "Processed 66000/40977 documents...\n",
      "Processed 68000/40977 documents...\n",
      "Processed 69000/40977 documents...\n",
      "Processed 70000/40977 documents...\n",
      "Processed 71000/40977 documents...\n",
      "Processed 72000/40977 documents...\n",
      "Processed 75000/40977 documents...\n",
      "Processed 76000/40977 documents...\n",
      "Processed 78000/40977 documents...\n",
      "Processed 85000/40977 documents...\n",
      "Processed 87000/40977 documents...\n",
      "Processed 90000/40977 documents...\n",
      "Processed 91000/40977 documents...\n",
      "Processed 92000/40977 documents...\n",
      "Processed 95000/40977 documents...\n",
      "Processed 96000/40977 documents...\n",
      "\n",
      "Generated a total of 40977 chunks.\n",
      "\n",
      "--- Example of a single processed document ---\n",
      "Customer ID: 7c396fd4830fd04220f754e42b4e5bff\n",
      "Chunk Text: Customer 7c396fd4830fd04220f754e42b4e5bff from sao paulo, SP placed an order on October 02, 2017. The order status is 'delivered'. The customer left a review with a score of 4 out of 5. The review comment is: 'Não testei o produto ainda, mas ele veio correto e em boas condições. Apenas a caixa que veio bem amassada e danificada, o que ficará chato, pois se trata de um presente.'\n",
      "Embedding Shape: (768,)\n",
      "\n",
      "Chunks and embeddings saved to '../data\\customer_embeddings.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "# Chunking\n",
    "documents_to_embed = []\n",
    "print(\"\\nStarting chunking and embedding process...\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # Split the document into chunks\n",
    "    chunks = text_splitter.split_text(row['text_document'])\n",
    "    \n",
    "    # Generate an embedding for each chunk\n",
    "    chunk_embeddings = embedding_model.encode(chunks)\n",
    "    \n",
    "    # Store the results\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        documents_to_embed.append({\n",
    "            'customer_unique_id': row['customer_unique_id'],\n",
    "            'order_id': row['order_id'],\n",
    "            'chunk_text': chunk,\n",
    "            'embedding': chunk_embeddings[i]\n",
    "        })\n",
    "    \n",
    "    if (index + 1) % 1000 == 0:\n",
    "        print(f\"Processed {index + 1}/{len(df)} documents...\")\n",
    "\n",
    "print(f\"\\nGenerated a total of {len(documents_to_embed)} chunks.\")\n",
    "\n",
    "# --- 5. Inspect and Save ---\n",
    "print(\"\\n--- Example of a single processed document ---\")\n",
    "example = documents_to_embed[0]\n",
    "print(\"Customer ID:\", example['customer_unique_id'])\n",
    "print(\"Chunk Text:\", example['chunk_text'])\n",
    "print(\"Embedding Shape:\", example['embedding'].shape) # Will be (768,) for this model\n",
    "\n",
    "# Save the final list of chunks and embeddings to a file\n",
    "with open(EMBEDDINGS_FILE, 'wb') as f:\n",
    "    pickle.dump(documents_to_embed, f)\n",
    "    \n",
    "print(f\"\\nChunks and embeddings saved to '{EMBEDDINGS_FILE}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8a2ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "my_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32, device=\"cpu\")\n",
    "print(my_tensor)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b79e9",
   "metadata": {},
   "source": [
    "## Setup DB and store chunked data\n",
    "- store chunked data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7798489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from '../data\\customer_embeddings.pkl'...\n",
      "Loaded 40977 document chunks.\n",
      "ChromaDB collection 'customer_reviews' is ready.\n",
      "\n",
      "Adding documents to the collection in batches of 5000...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metadatas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAdding documents to the collection in batches of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(documents), batch_size):\n\u001b[32m     49\u001b[39m     collection.add(\n\u001b[32m     50\u001b[39m         ids=ids[i:i + batch_size],\n\u001b[32m     51\u001b[39m         documents=documents[i:i + batch_size],\n\u001b[32m     52\u001b[39m         embeddings=[e.tolist() \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m [d[\u001b[33m'\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m docs_to_embed[i:i + batch_size]]],\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m         metadatas=\u001b[43mmetadatas\u001b[49m[i:i + batch_size]\n\u001b[32m     54\u001b[39m     )\n\u001b[32m     55\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAdded batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi//batch_size\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(documents)-\u001b[32m1\u001b[39m)//batch_size\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'metadatas' is not defined"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import os\n",
    "import pickle\n",
    "import uuid \n",
    "import time\n",
    "\n",
    "DATA_DIR = '../data'\n",
    "DB_DIR = '../db' \n",
    "EMBEDDINGS_FILE = os.path.join(DATA_DIR, 'customer_embeddings.pkl')\n",
    "COLLECTION_NAME = 'customer_reviews'\n",
    "\n",
    "print(f\"Loading embeddings from '{EMBEDDINGS_FILE}'...\")\n",
    "with open(EMBEDDINGS_FILE, 'rb') as f:\n",
    "    docs_to_embed = pickle.load(f)\n",
    "print(f\"Loaded {len(docs_to_embed)} document chunks.\")\n",
    "\n",
    "client = chromadb.PersistentClient(path=DB_DIR)\n",
    "\n",
    "collection = client.get_or_create_collection(name=COLLECTION_NAME)\n",
    "print(f\"ChromaDB collection '{COLLECTION_NAME}' is ready.\")\n",
    "\n",
    "ids = []\n",
    "documents = []\n",
    "metadata = []\n",
    "\n",
    "for doc in docs_to_embed:\n",
    "    # Each document needs a unique ID.\n",
    "    ids.append(str(uuid.uuid4())) \n",
    "    documents.append(doc['chunk_text'])\n",
    "    metadata.append({\n",
    "        'customer_id': doc['customer_unique_id'],\n",
    "        'order_id': doc['order_id']\n",
    "    })\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    \n",
    "batch_size = 5000\n",
    "print(f\"\\nAdding documents to the collection in batches of {batch_size}...\")\n",
    "\n",
    "for i in range(0, len(documents), batch_size):\n",
    "    collection.add(\n",
    "        ids=ids[i:i + batch_size],\n",
    "        documents=documents[i:i + batch_size],\n",
    "        embeddings=[e.tolist() for e in [d['embedding'] for d in docs_to_embed[i:i + batch_size]]],\n",
    "        metadatas=metadatas[i:i + batch_size]\n",
    "    )\n",
    "    print(f\"Added batch {i//batch_size + 1}/{(len(documents)-1)//batch_size + 1}\")\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "\n",
    "print(\"\\nAll documents have been added to the ChromaDB collection.\")\n",
    "print(f\"Total documents in collection: {collection.count()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8698ca9",
   "metadata": {},
   "source": [
    "## Test Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4140b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Running a test query ---\")\n",
    "query_text = \"qual foi a reclamação do cliente?\" # Portuguese for \"what was the customer's complaint?\"\n",
    "\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "print(\"Query Results:\")\n",
    "for i, doc in enumerate(results['documents'][0]):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(\"Text:\", doc)\n",
    "    print(\"Metadata:\", results['metadatas'][0][i])\n",
    "    print(\"Distance:\", results['distances'][0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea38dc",
   "metadata": {},
   "source": [
    "# RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2057b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "print(\"API Key loaded.\")\n",
    "\n",
    "\n",
    "DB_DIR = '../db'\n",
    "COLLECTION_NAME = 'customer_reviews'\n",
    "EMBEDDING_MODEL_NAME = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=DB_DIR,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "print(f\"Connected to ChromaDB collection '{COLLECTION_NAME}'.\")\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5}) # Retrieve top 5 most relevant chunks\n",
    "print(\"Retriever created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c99632",
   "metadata": {},
   "source": [
    "## Templating Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f15ce222",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatPromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# This template structures how we'll present the retrieved documents and the user's question to the LLM.\u001b[39;00m\n\u001b[32m      2\u001b[39m template = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33mYou are an expert assistant for a Brazilian e-commerce company.\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33mAnswer the user\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms question based ONLY on the following context.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[33mAnswer:\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m prompt = \u001b[43mChatPromptTemplate\u001b[49m.from_template(template)\n",
      "\u001b[31mNameError\u001b[39m: name 'ChatPromptTemplate' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# This template structures how we'll present the retrieved documents and the user's question to the LLM.\n",
    "template = \"\"\"\n",
    "You are an expert assistant for a Brazilian e-commerce company.\n",
    "Answer the user's question based ONLY on the following context.\n",
    "If the context doesn't contain the answer, say you don't have enough information.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26151a75",
   "metadata": {},
   "source": [
    "## Create RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247fff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG chain created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da50f444",
   "metadata": {},
   "source": [
    "## Test RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac7e003",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Testing the RAG chain ---\")\n",
    "\n",
    "test_customer_id = '861eff4711a542e4b93843c6dd7febb0'    # This is real id from our dataset\n",
    "\n",
    "question = f\"What was the review score and comment for the customer {test_customer_id}?\"\n",
    "\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"\\nAnswer: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
